---
title: "Résumé cours Classification"
author: "Augustin Antier - Louis Belle - Erwan Gouhier - Riwal Le Moan Delalande - Louis Prusiewicz "
date: "2025-11-14"
output:
  html_document:
    self_contained: true
    toc: true
    toc_depth: 2
    toc_float: true
editor_options: 
  markdown: 
    wrap: 72
---

```{=html}
<style>
@import url('https://fonts.googleapis.com/css2?family=Lora&family=Raleway:wght@400;500;600;800&display=swap');
@import url("https://fonts.googleapis.com/css2?family=Alegreya+Sans:ital,wght@0,100;0,300;0,400;0,500;0,700;0,800;0,900;1,100;1,300;1,400;1,500;1,700;1,800;1,900&family=Manrope:wght@200..800&family=Oswald:wght@200..700&family=Roboto+Slab:wght@100..900&display=swap" rel="stylesheet");

/* Définition des couleurs Violet/Neutre :
- #5A3E7B : Violet Profond (Titres H1)
- #937DC2 : Mauve Moyen (Titres H2/Liens)
- #3A3A3A : Gris Neutre Foncé (Corps du texte/Titres H3)
- #EDE8F5 : Mauve très clair (Arrière-plan Boîte)
- #7D6C98 : Violet pour survol (Liens survolés)
*/

body {
 font-family: "Alegreya Sans", sans-serif; /* Remplacé monospace par sans-serif pour Alegreya */
 font-weight: 400;
 font-style: normal;
 line-height: 1.6;
 color: #3A3A3A; /* Gris neutre foncé pour le texte */
 background-color: #FFFFFF; /* Fond blanc neutre */
 margin: 0;
 padding: 0;
}


h1 {
 font-family: 'Raleway', sans-serif;
 font-weight: 800;
 font-size: 3em;
 color: #5A3E7B; /* Violet Profond */
 letter-spacing: 1px;
 margin-top: 0.5em;
 border-bottom: 2px solid #EDE8F5; /* Petite ligne sous H1 */
 padding-bottom: 5px;
}

h2 {
 font-family: 'Raleway', sans-serif;
 font-weight: 600;
 font-size: 1.8em;
 color: #937DC2; /* Mauve Moyen */
 letter-spacing: 0.5px;
 margin-top: 1.5em;
}

h3 {
 font-family: 'Raleway', sans-serif;
 font-weight: 500;
 font-size: 1.4em;
 color: #3A3A3A; /* Gris Neutre Foncé */
 margin-top: 1em;
}

h4, h5 {
 counter-reset: none;
}

h4 > .header-section-number,
h5 > .header-section-number {
 display: none;
}


.box {
 background-color: #EDE8F5; /* Mauve très clair pour l'arrière-plan de la boîte */
 border: 1px solid #937DC2; /* Bordure simple Mauve Moyen */
 border-left: 5px solid #5A3E7B; /* Bordure latérale pour l'accent */
 border-radius: 5px;
 padding: 20px;
 margin: 15px 0;
 box-shadow: 0 2px 5px rgba(0, 0, 0, 0.05); /* Ombre très légère */
}

#TOC a {
 color: #937DC2; /* Mauve Moyen */
 text-decoration: none;
 font-weight: 500;
}

#TOC a:hover {
 text-decoration: underline;
 color: #7D6C98; /* Violet pour survol */
}

#TOC .tocify-header {
 color: #5A3E7B; /* Violet Profond */
 font-weight: 700;
 margin-top: 1em; /* espace AVANT le titre */
 margin-bottom: 0.2em; /* petit espace APRÈS */
}

#TOC .tocify-subheader {
 color: #937DC2; /* Mauve Moyen */
 font-weight: 500;
 margin-top: 0.1em;
 margin-bottom: 0.1em;
 margin-left: 1.2em;
}

</style>
```

# Introduction

Dans ce cours, nous allons explorer différentes méthodes de
classification non supervisée. Cette approche est utilisée lorsque, a
priori, la structure des groupes (ou classes) dans la population n'est
pas connue. Contrairement à la classification supervisée, l'objectif
n'est pas de prédire des étiquettes existantes, mais de mettre en
évidence une structure latente dans les données.

Cette structure peut prendre deux formes principales :

**Partition stricte** : Un découpage où chaque observation appartient
exclusivement à un seul groupe (comme la classification hiérarchique ou
les K-means).

**Classification floue** (ou probabiliste) : Où chaque observation se
voit attribuer une probabilité d'appartenance à différentes classes
(comme les modèles de lois de mélanges).

Nous étudierons comment classifier une grande variété d'objets : des
variables, des individus, des distributions, et même des données
textuelles et fonctionnelles.

## Plan

**1 - Classification de variable**\
Cette première section est dédiée aux concepts de base nécessaires à
toute classification :

Définition des objets : Clarifier la nature des éléments que nous
cherchons à classer (individus, variables, ou autres structures
complexes).

Mesure de distance : Définir ce que signifie être "proche" ou "éloigné"
dans l'espace des données, un choix fondamental qui influence le
résultat de la classification.

Critères d'agrégation et d'arrêt : Déterminer les règles pour former ou
fusionner des classes, et quand mettre fin au processus de
classification hiérarchique.

**2- Problème du 4eme coin ?**\
C'est un problème très présent en biologie mais que l'on retrouve dans
d'autre discipline. Pour résoudre ce problème une méthode de
classification ascendente hiérarchique sera mise en place.

**3 - Loi de mélanges**\
Cette partie introduit les concepts de lois de mélanges, en se
concentrant sur les gaussiennes multivariées. Ces distributions sont un
outil clé pour la classification probabiliste, car elles permettent de
modéliser des populations hétérogènes.

**4- Algorithme EM**\
Nous détaillerons le fonctionnement de l'algorithme EM
(Expectation-Maximization) et son application pour estimer les
paramètres des lois de mélanges. Pour consolider la compréhension des
méthodes itératives de partitionnement, nous étudierons également
l'algorithme des K-means, qui est souvent présenté comme un cas
particulier de l'EM sous certaines conditions

**5- Variables fonctionelles**\
Les variables fonctionnelles sont particulières, ce sont des courbes ou
des fonctions. On peut donc en extraire des features facilitant la
classifications de celles-ci.

**6- Topic modeling**\

Le point culminant de ce cours sera l'étude et la mise en œuvre d'une
méthode de Topic Modeling. L'objectif final est de comprendre les
principes algorithmiques et les étapes pratiques permettant d'identifier
les thèmes latents dans un corpus de documents textuels.

# Cours 1 - Calcul matriciel

## Classification de variables

Pour tout problème il est essentiel de savoir quel est **la sortie** que
nous attendons ainsi que notre **point de départ**. Dans le cadre de la
classification de variables on dispose d'une matrice de dimension n\*p
(n : nombre d'individus ; p : nombre de variable). L'objectif est
d'obtenir une matrice de distance entre nos variables deux à deux. Cette
**matrice D** serait donc de dimension p\*p.

### Qu'est ce qu'une distance ?

Il est nécessaire de réfléchir à ce que signifie la notion de distance
entre deux variables. Dans notre cas nous avont choisi comme définition
de la distance de variable l'écart orthogonal qui varie entre 0 et 1.

Cette définition de la distance ce traduit par : - **Deux variables
proches** sont deux variables qui évoluent de manière conjointe
positivement ou négativement et qui ont une distance proche de 0. -
**Deux variables eloignées** sont deux variables orthogonales qui
évoluent de manière disjointe ou indépendante. Elles ont une distance
proche de 1.

### Calcul

```{r, include=FALSE}
#| echo: false
rm(list = ls())

```

Pour l'exemple nous allons utiliser le jeu de données decathlon du
package FactoMineR.

```{r}
library(FactoMineR)
data(decathlon)
decat <- decathlon[,1:10] #Importation du jeux de données 

n = nrow(decat)
p = ncol(decat)

# Création de la matrice centrée 

moy <- matrix(1/n, nrow = n, ncol = n) %*% as.matrix(decat)
decat_c <- as.matrix(decat) - moy

# Création de la matrice centrée réduite

var_cov_decat <- t(decat_c) %*% decat_c

sig_decat <- 1/sqrt(1/(n-1)*diag(var_cov_decat))
diagsig <- diag(sig_decat)

decat_cr <- decat_c %*% diagsig

# Création de la matrice des corrélations r
cor_decat_cr <- 1/(n-1) * (t(decat_cr)  %*% decat_cr)

# Calcul que la matrice de distance 1-r^2
d = matrix(1, nrow = p, ncol = p) - cor_decat_cr**2

d <- round(d,2)

row.names(d) <- names(decat)
colnames(d) <- names(decat)
```

La distance $\mathbf{D}_{ij} = 1 - \mathbf{r}_{ij}^2$ mesure la
redondance informationnelle. Le coefficient de détermination
$\mathbf{r}^2$ représente la proportion de variance commune entre deux
variables. Par conséquent, minimiser cette distance revient à maximiser
la variance partagée (ou la colinéarité). Cela garantit que les
variables fortement corrélées (que ce soit positivement ou
négativement), qui mesurent essentiellement la même dimension latente,
seront regroupées ensemble.

# Cours 2 - Problème du 4ème coin

```{r,include=FALSE}
#| echo: false
#| message: false
#| warning: false
rm(list = ls())
library(Factoshiny)
```

Classification ascendante hiérarchique sur un jeu de donnée particulier

Le jeu de donnée est composé de **3 blocs**. Le premier comporte 1063
consommateurs qui ont notés 30 saumons. Un second bloc comporte les
réponses qualitatives des consommateurs à des questions d'usage et
d'attitude (sur leur mode de consommation du saumon par exemple). Le
troisième bloc comporte pour chaque saumon des informations physico
chimiques. Les individus de ce bloc sont donc des variables.

Notre objectif est de faire le lien entre ces trois blocs de variables
pour voir si les consommateurs ont apprécié des saumons dont la réalité
physique (taux de sel, ph) correspond avec leurs habitudes de
consommations.

Cette méthode va donc nous permettre de vérifier de la **qualité de nos
données**.

```{r}
#| message: false
#| warning: false


dta <- read.table("saumon.csv", 
                  header = TRUE, 
                  stringsAsFactors = TRUE, 
                  sep = ";", 
                  dec = ",")

# Affichage des 5 premières et 5 dernières variables
summary(dta)[,c(1:5,95:100)] 

sum(is.na(dta[1:1063,3:32])) # Il y a 30 NA

# Après vérifications, il s'agit de l'individu 1003, 
# qui n'a donné aucune note hédonique pour les saumons. 
# On le supprime du jeu de donnée, il n'apporte pas d'informations

dta <- dta[-1003,]
sum(is.na(dta[1:1062,3:32]))

row.names(dta) <- dta$IKIDEN
```

Les données sont bien importées. On veut réaliser une classification
ascendante hiérarchique sur les consommateurs au regard de leur
appréciation des différents saumons (bloc rouge).

```{r}
#| message: false
#| warning: false


# ACP sur les données hédoniques
res_pca <- PCA(dta[1:1062,3:32], graph = FALSE)

# Affichage des graphs des individus et des variables
plot(res_pca, label = "none", col.ind = "red")
plot.PCA(res_pca, choix = "var")

# Classification ascendante hiérarchique
res_HCP<-HCPC(res_pca, nb.clust = 3)
```

Le clustering nous donne trois groupes de consommateurs différents. Sans
variables suplémentaire il est difficile d'interpréter ces groupes. On
réalise donc une ACP avec comme variables supplémentaires les variables
"usages et attitude" (bloc bleu), puis une classification. On regarde
ensuite la caractérisation des classes par ces variables
supplémentaires.

```{r}
#| message: false
#| warning: false


# ACP sur les données
res_pca2 <- PCA(dta[1:1063,3:100], quali.sup=c(31:98), graph = FALSE)

# Affichage des graphs des variables et des individus
plot(res_pca2, label = "none",col.quali ="blue",col.ind = "red")
# plot.PCA(res_pca2, choix = "var")

# Classification ascendante hiérarchique
res_HCP2 <- HCPC(res_pca2, nb.clust = 3, graph = FALSE)

# On regarde la caractérisation du groupe 3 par les variables 
res_HCP2$desc.var$category$`3`[1:5,]
```

L'objet **desc.var** des resultats de la fonction HCPC est un catdes
réalisé sur la variable de classe. - La **colonne Cla/Mod** indique le
pourcentage d'individue de la classe qui ont pris cette modalité. - La
**colonne Mod/Cla** indique le pourcentage d'individus qui ont pris
cette modalité qui sont la classe. - **Global** est le pourcentage
global d'individus qui ont pris cette modalité dans le jeu de données.

Le test perfomé sert à voir si la modalité est sur ou sous-représenté
parmi les individus de la classe.

**Le groupe 3** préfère consommer des saumons salés. Ils consomment le
saumon pour ses apsects bénéfique pour la santé (oméga 3). C'est un
goupe ou les pesonnes qui consomment le saumon sont principelement des
adultes. Ils apprécient le goût caractéristique du saumon et le consomme
plutôt seul (sans pain beurre ou citron).

On veut maintenant s'assurer que cette caractérisation des classes n'est
pas décorellée de la réalité physique des produits. On va donc projeter
les variables physico-chimiques (bloc noir) sur le nuage des individus.
Moralement, pour la variable "concentration en sel", la ligne devient
donc un individu aimant beaucoup les saumons salés, plutôt qu'une
description du salé d'un saumon. On veut donc que cet individu
supplémentaire soit proche d'un groupe qui est caractérisé comme aimant
les saumons salés, ici le groupe 3.

```{r}
#| message: false
#| warning: false


# Réalisation de l'ACP et affichage des graphs
res_pca3 <- PCA(dta[,3:100], quali.sup=c(31:98), 
                ind.sup = c(1063:1087), graph = FALSE)

plot.PCA(res_pca3, label = "ind.sup", col.ind = "red", 
         col.ind.sup = "black", col.quali ="blue")

plot.PCA(res_pca, choix = "var")

# Extraction des coordonnées des individus supplémentaires
saumon.coords <- res_pca3$ind.sup$coord
saumon.coord.df <- as.data.frame(saumon.coords)

# Classification ascendante hiérarchique
res_HCP3 <- HCPC(res_pca3, nb.clust = 3, graph = FALSE)

# Extraction des coordonnées des individus sur les dimensions de l'ACP 
# et de leur groupe après classification
clusters <- res_HCP3$desc.axes$call$X

# calcul des centres de gravités de chaques cluster
GC <- aggregate(clusters[, c("Dim.1", "Dim.2", "Dim.3", "Dim.4", "Dim.5")], 
                by = list(cluster = clusters$clust),FUN = mean)
GC

# On assigne un individu supplémentaire au cluster 
# dont il est le plus proche du centre de gravité
assign_clusters <- function(row, GC) {
  dists <- apply(GC[, 2:6], 1, function(center) sum((row - center)^2))^0.5
  return(which.min(dists))
}

# On effectue cette opération pour tous les individus
saumon.coord.df$cluster_dist <- apply(saumon.coord.df[, 1:5], 
                                      1, assign_clusters, GC = GC)

# On regarde à quel cluster sont associés les individus supplémentaires
head(saumon.coord.df)
```

Les points noir projeté comme des individus supplémentaire correspondent
dans notre espace à des individus particuliers. Par exemple pour le
point "salt", c'est un indivdus qui aime uniquement les produits salés.
Les individus réél (points rouges) proche de ce point devrait donc
apprécié les saumons salés.

On remarque que l'individu supplémentaire "salt" est assigné au groupe
3, caractérisé par une appétence pour les saumons salés. Cela nous
rassure quand à la consistance de nos données. Nos données nous sembles
donc relativement fiables puisque les préférences des consomamteurs sont
cohérentes avec la réalité physique de nos produits.

# Cours 3 - modèle de mélange

Une loi de mélange est définie comme une **combinaison linéaire** de
fonctions de densité de probabilité.

Les paramètres d'une **loi normale** (ou gaussienne) sont la moyenne
($\mu$) et la variance ($\sigma^2$), ou parfois l'écart-type ($\sigma$).

Les paramètres d'une **loi normale multidimensionnelle** (ou
multivariée) sont le vecteur de moyennes ($\boldsymbol{\mu}$) et la
matrice de variance-covariance ($\boldsymbol{\Sigma}$).

La **fonction de densité** d'une gaussienne s'écrit de la manière
suivante : $$
f(x; \mu, \sigma) = f(x) = \frac{1}{\sigma\sqrt{2\pi}}\exp\left(-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2\right)
$$

## Exemple de loi bi dimensionelles :

```{r,include=TRUE}
#| echo: false
#| message: false
#| warning: false

library(mvtnorm)       # Pour dmvnorm (Densité multivariée normale)
library(ggplot2)       # Pour les tracés 2D avec gradient
library(RColorBrewer) 

# Préparation de la grille
x <- seq(-3, 3, length.out = 50)
y <- seq(-3, 3, length.out = 50)
grid <- expand.grid(x = x, y = y)

# Calcul de la densité 
calculate_density <- function(mu, Sigma) {
  z <- dmvnorm(grid, mean = mu, sigma = Sigma)
  grid$Z <- z
  Z_matrix <- matrix(z, nrow = length(x), ncol = length(y))
  return(list(data_2d = grid, data_3d = Z_matrix))
}

#' Trace la densité 2D avec des courbes de niveau et un gradient de couleur
plot_bivariate_normal_2d <- function(data_2d, title) {
  p <- ggplot(data_2d, aes(x = x, y = y)) +
    # Image Raster avec Gradient de couleur
    geom_raster(aes(fill = Z), interpolate = TRUE) +
    # Courbes de Niveau (pour mieux voir la forme)
    geom_contour(aes(z = Z), color = "white", alpha = 0.5, bins = 10) +
    # Application du Gradient de couleur
    scale_fill_gradientn(
      colors = brewer.pal(9, "BuPu"),
      name = "Densité (Z)"
    ) +
    labs(title = paste("", title), x = "X", y = "Y") +
    coord_fixed(ratio = 1) + # Assure que les axes sont à la même échelle
    theme_minimal()
  
  print(p)
}

plot_bivariate_normal_3d_final <- function(Z, title) {
  # x est passé en argument pour s'assurer que la fonction a accès aux dimensions de la grille
  
  # 1. Définir la plage de Z pour la mise à l'échelle des couleurs
  z_min <- min(Z)
  z_max <- max(Z)
  
  # 2. Créer une palette de couleurs. (Rouge pour le MAX)
  color_palette <- RColorBrewer::brewer.pal(9, "BuPu")# Rouge en premier, Jaune en dernier
  color_func <- colorRampPalette(color_palette)(100) # 100 niveaux de couleurs
  
  # 3. Calculer les indices de couleur (1 à 100) pour chaque point Z
  # Attention : Z est une matrice, donc color_indices doit aussi être une matrice.
  color_indices_vector <- pmax(1, ceiling((Z - z_min) / (z_max - z_min) * 100))
  color_indices_matrix <- matrix(color_indices_vector, nrow = nrow(Z), ncol = ncol(Z))
  
  # 4. Assigner la couleur à chaque facette (LA CORRECTION EST ICI)
  # Pour persp, on utilise les indices de couleur de la matrice (dim: 50x50) 
  # et on sélectionne la sous-matrice de dimensions (49x49) pour les facettes.
  
  # Utilisation de l'indexation basée sur les limites [1:49, 1:49]
  fcol <- color_func[color_indices_matrix[1:(nrow(Z)-1), 1:(ncol(Z)-1)]]
  
  # 5. Tracé 3D avec persp()
  persp(x = x, y = x, z = Z, # Note: x est utilisé pour les deux axes si la grille est carrée
        theta = 30, phi = 30, expand = 0.5,
        col = fcol, 
        main = paste("", title),
        xlab = "X", ylab = "Y", zlab = "z",
        ticktype = "detailed",
        zlim = c(z_min, z_max)) 
}

```

### Variabilité isolée

```{r, include=TRUE}
#| echo: false
#| message: false
#| warning: false
mu_1 <- c(0, 0)
Sigma_1 <- matrix(c(1, 0, 0, 1), nrow = 2) # Sigma = I
title_1 <- "Variabilité isolée"

data_1 <- calculate_density(mu_1, Sigma_1)
plot_bivariate_normal_2d(data_1$data_2d, title_1)
plot_bivariate_normal_3d_final(data_1$data_3d, title_1)

```

Sur la fonction de densité (graphique 3D), on observe un pic centré dont
la forme est parfaitement circulaire à la base. En projection 2D
(courbes de niveau), ceci se traduit par des cercles concentriques.

Cette configuration signifie que la matrice de variance-covariance
($\boldsymbol{\Sigma}$) est **diagonale** (les éléments non diagonaux
sont nuls). Il en résulte une **covariance nulle** et donc une
corrélation nulle entre les variables $X$ et $Y$. Autrement dit, la
variable $X$ n'a aucune influence linéaire sur la variable $Y$, et
vice-versa. La variabilité est isotrope (uniforme dans toutes les
directions), ne permettant pas d'identifier d'axe privilégié de
dispersion.

### Covariance Positive

```{r, include=TRUE}
#| echo: false
#| message: false
#| warning: false
mu_2 <- c(0, 0)
Sigma_2 <- matrix(c(1, 0.7, 0.7, 1), nrow = 2) # Corrélation positive
title_2 <- "Covariance Positive"

data_2 <- calculate_density(mu_2, Sigma_2)
plot_bivariate_normal_2d(data_2$data_2d, title_2)
plot_bivariate_normal_3d_final(data_2$data_3d, title_2)
```

La forme de la fonction de densité 3D n'est plus un pic circulaire, mais
s'étire pour former une crête ressemblant à un "aileron de requin". En
2D, les courbes de niveau sont des ellipses clairement orientées selon
une diagonale montante (du quadrant inférieur gauche au supérieur
droit).

Ce motif indique une **covariance positive** significative entre $X$ et
$Y$. Les éléments non diagonaux de la matrice $\boldsymbol{\Sigma}$ sont
positifs. Cela signifie qu'une augmentation de la valeur de $X$ est
associée à une augmentation de la valeur de $Y$ (et inversement). La
corrélation entre les deux variables est positive.

### Vecteur de moyenne différent

```{r, include=TRUE}
#| echo: false
#| message: false
#| warning: false
mu_3 <- c(1.5, 1.0) # Décalage du centre
Sigma_3 <- Sigma_2 # Même corrélation positive
title_3 <- "Vecteur de moyenne différent"

data_3 <- calculate_density(mu_3, Sigma_3)
plot_bivariate_normal_2d(data_3$data_2d, title_3)
plot_bivariate_normal_3d_final(data_3$data_3d, title_3)
```

La fonction de densité 3D conserve sa forme d'aileron de requin,
indiquant que la structure de covariance (donc la corrélation) est
inchangée par rapport au cas précédent. Cependant, le centre de ce
sommet est décalé (translaté) dans l'espace.

Le déplacement du pic est uniquement dû à la modification du vecteur de
moyennes ($\boldsymbol{\mu}$), qui est passé de $(0, 0)$ à $(1.5, 1.0)$.
La variance et la covariance (la forme et l'orientation) sont
déterminées par la matrice $\boldsymbol{\Sigma}$ et restent identiques,
tandis que la position du centre (le point de densité maximale) est
définie par le vecteur $\boldsymbol{\mu}$.

### Covarience négative

```{r, include=TRUE}
#| echo: false
#| message: false
#| warning: false
mu_4 <- c(0, 0)
Sigma_4 <- matrix(c(1, -0.7, -0.7, 1), nrow = 2) # Corrélation négative
title_4 <- "Covariance Négative"

data_4 <- calculate_density(mu_4, Sigma_4)
plot_bivariate_normal_2d(data_4$data_2d, title_4)
plot_bivariate_normal_3d_final(data_4$data_3d, title_4)
```

L'aileron de requin s'est inversé. Il est maintenant orienté selon une
diagonale descendante (du quadrant supérieur gauche à l'inférieur
droit). En 2D, les ellipses suivent cette même orientation.

Cette orientation est la signature d'une **covariance négative**. Les
éléments non diagonaux de la matrice $\boldsymbol{\Sigma}$ sont
négatifs. Cela signifie qu'une augmentation de la valeur de $X$ est
associée à une diminution de la valeur de $Y$. La corrélation entre les
variables est négative.

## Comment généer un loi de mélange gaussienne

La génération d'une loi de mélange gaussienne se passe en 2 étapes :

-   **Choix de la classe** avec une probabilité à priori
    $\pi_k = P(C_k)$
-   **Génération de l'observation** : : Pour la classe $C_k$
    sélectionnée, on tire l'observation $X_i$ selon sa distribution
    Gaussienne correspondante, caractérisée par un vecteur des moyennes
    $\mu_k$ et une matrice de variance-covariance $\Sigma_k$.

## Estimer les paramètres d'une loi de mélange

Pour modéliser les données observées $\mathbf{X} = \{X_1, \ldots, X_n\}$
à l'aide d'une Loi de Mélange Gaussienne (GMM), notre objectif est
d'identifier l'ensemble des paramètres $\theta$ du modèle qui sont les
plus vraisemblables étant donné les données.Les paramètres $\theta$ que
nous cherchons à estimer sont : - Les **probabilités a priori**
($\pi_k$) des classes. - Le **vecteur des moyennes** ($\mu_k$) de chaque
composante Gaussienne. - La **matrice de variance-covariance**
($\Sigma_k$) de chaque composante Gaussienne.

La méthode utilisée pour trouver ces paramètres est le **Maximum de
Vraisemblance** (MV).

La fonction de vraisemblance $L(\mathbf{X}; \theta)$ est une mesure qui
quantifie la "plausibilité" des paramètres $\theta$ pour générer
l'ensemble de données $\mathbf{X}$ que nous avons observé.

$$
L(X_1, \ldots, X_n; \theta) = \prod_{i=1}^n f(X_i; \theta)
$$ Le Principe du MV consiste à trouver la valeur $\hat{\theta}$ qui
maximise cette fonction $L$.

Pour trouver le maximum de $L$, on cherche à résoudre
$\frac{\partial L}{\partial \theta} = 0$. Cependant, manipuler le
produit ($\prod$) est mathématiquement difficile.On utilise alors la
Log-Vraisemblance $\log(L)$ :Le passage au logarithme transforme le
produit en une somme (ce qui simplifie la dérivation).La fonction
$\log(x)$ est strictement croissante. Par conséquent, les paramètres
$\theta$ qui maximisent $L$ sont exactement les mêmes que ceux qui
maximisent $\log(L)$. Notre solution de Maximum de Vraisemblance
$\hat{\theta}$ ne change pas.

## Rappel sur Bayes Le théorème

de bayes permet de mettre à jour la probabilité d'une hypothèse C
(connaissance apriori) en sachant de nouvelles informations A.

On considère une partition $$
\{C_1, C_2, \ldots, C_K\}
$$ alors

$$
P(C_i|A) = \frac{P(A|C_i)P(C_i)}{\sum_{k=1}^K P(A|C_k)P(C_k)}
$$

# Cours 4 - Algorithme EM

Nous allons générer une **loi de mélange** de 2 gaussiennes
unidimensionelles. Avec l'algorithme EM codé à la main, on essaye de
retrouver les paramètres d'une loi de mélange générée par un autre
groupe. On gnénère notre vecteur avec le code suivant :

```{r,include=FALSE}
rm(list = ls())
library(tidyverse)
```

```{r}
n = 500 #Nombre d'individus 
mu1 = 0.7 # Moyenne de la première loi
mu2 = 1.3 # Moyenne de la deuxième loi
s1 = 0.1  # Ecart type de la première loi 
s2 = 0.2  # Ecart type de la deuxième loi
p1 = 0.6  # Probabilité d'appartenir à la première loi
p2 = 0.4  # Probabilité d'appartenir à la deuxième loi 

dt <- data.frame(x = c(rnorm(n * p1, mu1, s1), rnorm(n * p2, mu2, s2)))
dt <- data.frame(dt[sample(nrow(dt)),1])
colnames(dt) <- "x"

ggplot(dt) +
  aes(x = x) +
  geom_density()
```

## Importation des données du groupe

```{r}
dta <- read.table("res.csv", sep = ',', dec = '.', header = TRUE)
dta <- as.data.frame(dta[,-1])
dta <- rename(dta, "x" = "dta[, -1]")

ggplot(dta) +
  aes(x = x) +
  geom_density()
```

Une fois avoir importé les données de l'autre groupe, on réalise un
graphique de la densité pour avoir une idée de la distribution de leur
jeu de données.

## Algorithme Expection Maximisation (EM)

On commence par l'étape **d'exepection** (E) de l'algorithme EM. Cette
étape peut aussi être appelé étiquetage car c'est ici que l'on calcul la
probabiltié de nos points à appartenir à tel ou tel loi.

On estime les valeurs de départ de manière graphique. Après avoir tracer
la densité du jeu de données qui nous a été confié, on remarque deux
pics, qui vont correspondre a nos moyennes, on estime égallement un
écart type en se basant sur la largeur du pic. Pour la probabilité
d'appartenir à telle ou telle loi on commence par estimer 50/50.

```{r}
n = length(dta$x)

# Estimation des paramètres de la 1ere loi
Pc1 = 0.5
mu1 = -3
s1 = 4

# Estimation des paramètres de la 2eme loi
Pc2 = 0.5
mu2 = 15
s2 = 4

para = c(mu1, mu2, s1, s2, Pc1, Pc2)
```

Avant de se lancer dans l'algorithme en tant que tel on réalise les
calculs sur 3 points pour s'assurer que notre logique est la bonne. Les
points choisis sont les suivants : $x_{1} = 0, x_{2} = 5, x_{3} = 12$

```{r}
x1 = 0
x2 = 5
x3 = 12
```

On commence donc par l'étape **Expectation**. Pour chacun de nos points
on calcul la probabilité d'appartenance à chaque classes en fonction des
paramètres estimé graphiquement.

```{r}
# pour x1
Px1_c1 = dnorm(x1, mu1, s1)
Px1_c2 = dnorm(x1, mu2, s2)

Px1 = Px1_c1 * Pc1 + Px1_c2 * Pc2
Pc1_x1 = (Px1_c1 * Pc1) / Px1
Pc2_x1 = (Px1_c2 * Pc2) / Px1
print(Pc1_x1)
print(Pc2_x1)

# pour x2
Px2_c1 = dnorm(x2, mu1, s1)
Px2_c2 = dnorm(x2, mu2, s2)

Px2 = Px2_c1 * Pc1 + Px2_c2 * Pc2
Pc1_x2 = (Px2_c1 * Pc1) / Px2
Pc2_x2 = (Px2_c2 * Pc2) / Px2
print(Pc1_x2)
print(Pc2_x2)

# pour x3
Px3_c1 = dnorm(x3, mu1, s1)
Px3_c2 = dnorm(x3, mu2, s2)

Px3 = Px3_c1 * Pc1 + Px3_c2 * Pc2
Pc1_x3 = (Px3_c1 * Pc1) / Px3
Pc2_x3 = (Px3_c2 * Pc2) / Px3
print(Pc1_x3)
print(Pc2_x3)
```

## Réalisation de l'algorithme

Pour tout algorithme il faut définir un **critère de convergence**. Dans
notre cas nous avons choisi la **log vraissemblance**. Notre algorithme
s'arrête lorsque la log vraissemblence de l'étape précédente et celle de
l'étape actuelle sont proche. Il faut définir un seuil pour définir ce
que veux dire proche.

```{r}
prob <- function(mu1, mu2, s1, s2, p1, p2, x){
  px1 <- dnorm(x=x, mean=mu1, sd=s1) * p1
  px2 <- dnorm(x=x, mean=mu2, sd=s2) * p2
  px <- px1 + px2
  return(list(p1 = px1 / px, p2 = px2 / px))
}

# a <- data.frame(x = c(0, 6, 12))

# Création de la fonction réalisant l'étape E
E <- function(para, vec){
  p <- as.data.frame(prob( mu1 = para[1], mu2 = para[2], s1 = para[3], s2 = para[4], p1 = para[5], p2 = para[6], x = vec))
  return(p)
}

# proba <- E(para = para, vec = dta$x)
```

Une fois l'étape E programmée, il faut programmer l'étape de
**Maximisation**, M. Cette étape estime les nouveaux paramètres en se
basant sur les probabilité d'appartenance calculées à l'étape E.

Pour calculer mu on fait une moyenne de chaque points pondéré par leur
probabilité d'appartenance à la classe.

```{r}
M <- function(proba){
  Pc1 = mean(proba$p1)
  Pc2 = mean(proba$p2)


  mu1 = sum(proba$p1 * dta$x)/(n*Pc1) 
  mu2 = sum(proba$p2 * dta$x)/(n*Pc2)
  # mu1;mu2

  s1 = sqrt(sum((dta$x - mu1)**2 * proba$p1) / (n*Pc1-1))
  s2 = sqrt(sum((dta$x - mu2)**2 * proba$p2) / (n*Pc2-1))
  # s1 ; s2
  return(c(mu1, mu2, s1, s2, Pc1, Pc2))
}

# para <- M(proba)
```

On réalise nos deux étapes une fois pour s'assurer que nos fonctions
marchent.

```{r}
proba <- E(para = para, vec = dta$x)
para <- M(proba)
print(para)
```

On créer enfin la fonction du calcul de la log-vraissemblance.

```{r}

logLi <- function(x, param){
  log(dnorm(x, param[1], param[3])*param[5] + 
        dnorm(x, param[2], param[4])*param[6])
}
  
logL <- function(x, param){
  sum(apply(X = x, MARGIN = 1, FUN = logLi, param = param))
}

logL(x = dta, para)
```

On fait tourner notre algorithme 100 fois.

```{r}
logL(x = dta, para)
for (i in 1:100){
  proba <- E(para = para, vec = dta$x)
  para <- M(proba)
  #print(para)
  #print(logL(x = dta, para))
}
print(para)
```

# Cours 5 - classification de variables fonctionnelles

```{r,include=FALSE}
rm(list = ls())
library(tidyverse)
```

On charge d'abord nos données.

```{r}

dt <- read.csv("data.csv")
dt[] <- lapply(dt, function(x) gsub("\\.\\.", ".", x))
dt <- as.data.frame(sapply(dt, as.numeric))

```

On visualise les données. On remarque qu'elles représentent des courbes.
Ce sont donc des **données fonctionnelles**. Un exemple typique de
données fonctionnelles sont des courbes d'absorbtion. On remarque que
ces courbes ont trois points d'infléxions.

```{r}

dt_long <-  pivot_longer(dt, cols = starts_with("V"), names_to = "temps", values_to = "value")

dt_long$temps <- as.numeric(sub("V", "", dt_long$temps))

ggplot(dt_long, aes(x = temps, y = value, group = X)) +
  geom_line() 

```

La première idées est de réaliser une classification avec **K means** en
résumant les courbes par les pentes entre leurs points d'infléxions.

## Présentation de la méthode des k-means

La méthode des **kmeans** fait partie de la famille des algorithmes EM.
Cette méthode nécessite de connaitre a priori le nombre de classes que
l'on veut faire. C'est une méthode de partition stricte.

-   **Initialisation**, on choisi K barycentre.

-   **Expectetion**, on calcule la distance de chaque point au K
    barycentre et on l'assigne au groupe dont il est le plus proche.

-   **Maximisation**, on recalcule les barycentre des groupes.

On recommence jusqu'à convergence. La convergence est atteinte quand les
barycentres ne bougent plus entre 2 itérations.

```{r}

temps1 <- as.numeric(dt[,"V33"]) - as.numeric(dt[,"V1"])
temps2 <- as.numeric(dt[, "V66"]) -  as.numeric(dt[, "V34"])
temps3 <- as.numeric(dt[, "V67"]) -  as.numeric(dt[, "V100"])

df <- data.frame(temps1 = temps1, temps2 = temps2, temps3 = temps3)


cl <- kmeans(df, centers = 3, iter.max = 20, nstart = 2)

dt_clust <- dt
dt_clust$clust <- cl$cluster


```

Pour estimer la pente de nos courbes, on utilise des **régressions
linéaires**. On calcule la pente entre chaque point d'inflexion, il y a
donc 3 coefficients directeurs par courbe. L'utilisation des
coefficiants dirrecteurs pour la classification correspond à une
**extraction de features**.

```{r}
get_slope <- function(x) {
  model <- lm(x ~ time)
  coef(model)[2]  # le coefficient directeur (slope)
}


dt_1 <- dt[, 2:34]
time <- 1:ncol(dt_1)

a1 <- apply(dt_1, 1, get_slope)

dt_2 <- dt[, 35:67]
time <- 1:ncol(dt_2)

a2 <- apply(dt_2, 1, get_slope)

dt_3 <- dt[, 68:101]
time <- 1:ncol(dt_3)

a3 <- apply(dt_3, 1, get_slope)

a <- data.frame(n = 1:150, a1 = a1, a2 = a2, a3 = a3)


```

```{r}

obs <- as.vector(dt[1, -1])
estim <- cumsum(c(as.numeric(dt[1, -1][1]), rep(a[1, 2], 33), rep(a[1, 3], 33), rep(a[1, 4], 33)))

diff_obs_estim <- data.frame(obs = obs, estim = estim) 

df_visu <- data.frame(cbind(obs, estim, x = 1:100))
df_visu$obs <- as.numeric(df_visu$obs)
df_visu$estim <- as.numeric(df_visu$estim)
df_visu$x <- as.numeric(df_visu$x)
ggplot(df_visu) + 
  geom_line(aes(x = x, y = obs)) + 
  geom_line(aes(x = x, y = estim))


```

Ce graphique permet de montrer que notre estimation des courbes par
leurs features n'est pas déconant.

On fait un clustering avec la méthode des kmeans comme évoqué
précedement.

```{r}

cl <- kmeans(a[, 1:3], centers = 3, iter.max = 20, nstart = 2)
dt_clust2 <- dt
dt_clust2$cluster <- cl$cluster

```

On représente grapgiquement les coeficients directeurs sur les trois
zones en fonction des clusters.

```{r}

a$cluster <- as.factor(dt_clust2$cluster)
a_long <- pivot_longer(a, cols = starts_with("a"), names_to = "temps", values_to = "value")

ggplot(a_long, aes(x = temps, y = value, col = cluster)) +
  geom_point(alpha = 0.5) +
  facet_wrap(~cluster)


```

On représente les courbes en les colorant par leur cluster.

```{r}

dt$cluster <- as.factor(cl$cluster)

dt_long_c <-  pivot_longer(dt, cols = starts_with("V"), names_to = "temps", values_to = "value")

dt_long_c$temps <- as.numeric(sub("V", "", dt_long_c$temps))

ggplot(dt_long_c, aes(x = temps, y = value, group = X, colour = cluster)) +
  geom_line() 

```

Pour comparaison, on effectue une classification à l’aide du package
**mclust**, spécialisé dans la **classification et la modélisation de
données selon des mélanges de lois normales** (Gaussian Mixture Models).
Ce package détermine automatiquement le **nombre optimal de classes** et
les **paramètres des distributions** associées à chaque cluster.

```{r}
library(mclust)
res_mclust<-Mclust(a[, 1:3]) #réalisation de la classification
res_mclust$classification #affiche de la classe des individus
```

Le résultat obtenu avec *mclust* est très proche de la classification de
référence. Toutefois, le modèle crée une classe supplémentaire (la
classe numéro 2) qui regroupe les individus présentant les pentes les
plus importantes, indiquant une sous-structure détectée automatiquement
par l’algorithme.

# Cours 6 - Topic Modelling

Le **Topic modeling** est une méthode de NLP (natural language
processing). C'est une exploration non supervisé d'un ensemble de textes
(appelés **documents** par la suite), afin de produir un ensemble de
sommaire de **termes** (mots) représentant l'ensmble des **thèmes
principaux** (Topics) abordés dans le corpus de document.

Nous nous sommes basés sur l'article "Latent Dirichlet Allocation" de
David M.BLei.

Lors de la lecture d'un article scientifique sur les statistique la
première question à se poser c'est quel est notre **objet statistique**.
Dans notre cas l'objet statistique d'intérêt c'est le document.

La première question à se poser c'est qu'est ce qu'un "document" ?

Un **document** est composé de plusieurs Topics. Chaque Topics est une
**distribution de mots**. La difficulté par rapport aux lois de mélange
c'est qu'on ne calcul plus une probabilité d'appartenance mais une
distribution (une probabilité de probabilité) car un document est
composé de plusieurs Topics.

Une fois avoir identifié ce qu'est un document, il faut se questionner
sur le processus de génération. Nous sommes dans un exercice de
modélisation et pour modéliser il faut comprendre comment on génère un
document.

Comment on modélise un "document" ? Quel est le processus génératif ?

On fixe nos hyperparamètres K (le nombre de topics) $\alpha$ et $\beta$

$\alpha$ et $\beta$ traduisent la poly thématique des documents ainsi
que la richesse lexicale des topics.

Nous avons trois objets principaux :

Doc \<--\> Topics \<--\> Mots

Le processus de génération est **hiérarchique**.

La première étape consiste à créer les topics en leur attribuants des
mots.

Ensuite on se place à l'échelle du document, pour le premier mots du
document, on choisi un Topic dans la distrubution de Topics du
documents. Une fois le Topics selectionné on dire un mot dans la
distribution de mot, c'est ce mot tiré qui sera notre premier mot. Ce
procesus est répété pour chaque mot du document.

Dans LDA il y a **Dirichlet**, mais qu'est ce qu'une loi de Dirichlet ?
Pour mieux comprendre on peut se placer dans le cas de K=2, c'est à dire
avec deux Topics. Dans ce cas, on suit une **loi beta**.

Le document 1 à 52% de Topic 1 et 48% de Topic 2 Le document 2 à 51% de
Topic 1 et 49% de Topic 2

Le lien entre Loi Beta et dirichlet est lié au lien entre Binomial et
Multinomial.

La Loi Beta est une probabilité de probabilité, c'est à dire une
distribution. La loi de Dirichlet est donc la généralisation de la loi
Beta pour une problème a plus de deux classes.

La dernière question que l'on se pose c'est comment la machine fait pour
faire le Topic modeling. Pour celà nous alons faire un exemple simple

Doc1 : Une *banque* me *pête* de l'*argent* pour une *dette* 

Doc2 : La *banque* *investit* dans la *dette* de l'*état* 

Doc3 : Le *poisson* est bon pour la *santé* 

Doc4 : *Manger* du *poisson* est bon pour la *santé*


La première étape consiste à enlever les mots qui ne portent pas de
sens.

Ensuite on mets les mots dans des Topics au hasard. On sait combien il y
a de Topics car on a fixé nos hyper paramètres.

T1 : banque, prête, poisson, manger .... 

T2 : Banque, dette, investit, santé ........

Ici Banque est dans 2 Topics différents. C'est car la machine ne fait
pas le liens entre les deux mots, elle les prends comme deux entités
différentes.

Une fois les Topics "initialisé" il faut réfléchir à comment on fait
pour les affiner.

L'algorithme EM ne marche pas car on ne peut pas maximiser ou minimiser,
dans ce mode opératoire on veut que ça converge, c'est à dire que les
mots ne changent plus de Topics (ou seulement à la marge).

Pour affiner les Topics, on va sortir un mot d'un topic, et le réafecter
en fonction de **critère d'affinité**.

Il nous en faut deux un pour le Topic et un pour le document. On veut
qu'un mot soit avec les mots utilisé dans le documents mais aussi des
mots du même Topics.

Ensuite on calcul le score d'affinité dans chaque Topics par exemple 7
pour K1 2 pour K2

Le mot a donc 7 chances sur 9 d'être dans le Topic 1, mais vu que ce
n'est pas un processus déterministe il a quand même 2 chance sur 9
d'étre dans le Topic 2.
